import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_teddynote.prompts import load_prompt
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_teddynote import logging
from dotenv import load_dotenv
import os
import re
import datetime
import json

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
logging.langsmith("[Project] PDF RAG")

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

# í”¼ë“œë°± ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache/feedback"):
    os.mkdir(".cache/feedback")

st.title("PDF ê¸°ë°˜ QAğŸ’¬")

# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["messages"] = []

if "chain" not in st.session_state:
    # ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•Šì„ ê²½ìš°
    st.session_state["chain"] = None

# ë¬¸ì„œ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_documents(docs):
    """ë¬¸ì„œ ì „ì²˜ë¦¬ - ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±° ë° í’ˆì§ˆ í–¥ìƒ"""
    processed_docs = []
    for doc in docs:
        content = doc.page_content
        
        # 1. ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        content = re.sub(r'\n\s*\n+', '\n\n', content)
        content = re.sub(r'\s+', ' ', content)
        
        # 2. í˜ì´ì§€ ë²ˆí˜¸, í—¤ë”/í‘¸í„° ì œê±°
        content = re.sub(r'í˜ì´ì§€\s*\d+', '', content)
        content = re.sub(r'Page\s*\d+', '', content)
        content = re.sub(r'^\d+\s*$', '', content, flags=re.MULTILINE)
        
        # 3. íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ë‹¨, ì˜ë¯¸ìˆëŠ” êµ¬ë‘ì ì€ ë³´ì¡´)
        content = re.sub(r'[â€¢â—¦â–ªâ–«â– â–¡â—â—‹â—†â—‡â–²â–³â–¼â–½]', '- ', content)
        
        # 4. URL ë° ì´ë©”ì¼ ì •ë¦¬
        content = re.sub(r'http[s]?://\S+', '[URL]', content)
        content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', content)
        
        # 5. ë‚´ìš© ì •ë¦¬ ë° ê²€ì¦
        content = content.strip()
        
        # 6. ìµœì†Œ ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸)
        if len(content) > 30 and not content.isspace():
            doc.page_content = content
            processed_docs.append(doc)
    
    return processed_docs

# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜
def create_hybrid_retriever(split_documents, embeddings):
    """ë²¡í„° ê²€ìƒ‰ê³¼ BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìƒì„±"""
    
    # 1. ë²¡í„° ê²€ìƒ‰ ì„¤ì •
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)
    vector_retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 4,  # í•˜ì´ë¸Œë¦¬ë“œì—ì„œëŠ” ê°ê° ì ì€ ìˆ˜ë¡œ
            "lambda_mult": 0.5,
            "fetch_k": 15
        }
    )
    
    # 2. BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ ì„¤ì •
    bm25_retriever = BM25Retriever.from_documents(split_documents)
    bm25_retriever.k = 4  # ë²¡í„° ê²€ìƒ‰ê³¼ ë™ì¼í•œ ìˆ˜
    
    # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì•™ìƒë¸”)
    ensemble_retriever = EnsembleRetriever(
        retrievers=[vector_retriever, bm25_retriever],
        weights=[0.7, 0.3]  # ë²¡í„° ê²€ìƒ‰ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
    )
    
    return ensemble_retriever

# Query Expansion ê¸°ëŠ¥ ì™„ì „ ì œê±° - ì•ˆì •ì„±ì„ ìœ„í•´

# ê¸°ë³¸ ì²­í‚¹ë§Œ ì‚¬ìš© - ë³µì¡í•œ ê³„ì¸µì  ì²­í‚¹ ì œê±°
def create_simple_chunks(docs):
    """ê¸°ë³¸ ì²­í‚¹ë§Œ ì‚¬ìš© - ì•ˆì •ì„± ìµœìš°ì„ """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    return text_splitter.split_documents(docs)

# ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥ ì™„ì „ ì œê±° - ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ë§Œ ì‚¬ìš©

# Reranking ê¸°ëŠ¥ ì™„ì „ ì œê±° (ì•ˆì •ì„±ì„ ìœ„í•´)
# ë³µì¡í•œ ì˜ì¡´ì„±ê³¼ í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¸í•´ ì œê±°ë¨

# ë‹µë³€ ê²€ì¦ í•¨ìˆ˜ë“¤
def validate_answer_quality(question, answer, context):
    """ë‹µë³€ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ ê²€ì¦"""
    quality_score = 0
    issues = []
    
    # 1. ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ì§€)
    if len(answer) < 50:
        issues.append("ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
    elif len(answer) > 3000:
        issues.append("ë‹µë³€ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤")
    else:
        quality_score += 1
    
    # 2. êµ¬ì¡° ì²´í¬ (í˜•ì‹ì´ ì˜ ê°–ì¶°ì ¸ ìˆëŠ”ì§€)
    if "### ğŸ“‹ ìš”ì•½" in answer and "### ğŸ“– ìƒì„¸ ë‹µë³€" in answer:
        quality_score += 1
    else:
        issues.append("ë‹µë³€ í˜•ì‹ì´ ë¶ˆì™„ì „í•©ë‹ˆë‹¤")
    
    # 3. ë‚´ìš© ê´€ë ¨ì„± ì²´í¬ (ì§ˆë¬¸ê³¼ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€)
    question_keywords = set(question.lower().split())
    answer_keywords = set(answer.lower().split())
    common_keywords = question_keywords.intersection(answer_keywords)
    
    if len(common_keywords) >= 2:
        quality_score += 1
    else:
        issues.append("ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ì—°ê´€ì„±ì´ ë‚®ìŠµë‹ˆë‹¤")
    
    # 4. "ëª¨ë¥´ê² ë‹¤" ë¥˜ì˜ ë‹µë³€ ì²´í¬
    uncertain_phrases = ["ëª¨ë¥´ê² ", "í™•ì‹¤í•˜ì§€ ì•Š", "ì°¾ì„ ìˆ˜ ì—†", "ëª…í™•í•˜ì§€ ì•Š"]
    if any(phrase in answer for phrase in uncertain_phrases):
        issues.append("ë¶ˆí™•ì‹¤í•œ ë‹µë³€ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
    else:
        quality_score += 1
    
    return {
        "score": quality_score,
        "max_score": 4,
        "issues": issues,
        "quality": "ë†’ìŒ" if quality_score >= 3 else "ë³´í†µ" if quality_score >= 2 else "ë‚®ìŒ"
    }

# í”¼ë“œë°± ë¡œê¹… í•¨ìˆ˜ë“¤
def log_feedback(feedback_type, question, answer, details=None):
    """í”¼ë“œë°±ì„ íŒŒì¼ì— ì €ì¥"""
    feedback_data = {
        "timestamp": datetime.datetime.now().isoformat(),
        "type": feedback_type,
        "question": question,
        "answer": answer,
        "details": details
    }
    
    # íŒŒì¼ëª…ì— ë‚ ì§œ í¬í•¨
    date_str = datetime.datetime.now().strftime("%Y%m%d")
    feedback_file = f".cache/feedback/feedback_{date_str}.jsonl"
    
    # JSONL í˜•ì‹ìœ¼ë¡œ ì¶”ê°€
    with open(feedback_file, "a", encoding="utf-8") as f:
        f.write(json.dumps(feedback_data, ensure_ascii=False) + "\n")

def show_feedback_stats():
    """í”¼ë“œë°± í†µê³„ í‘œì‹œ"""
    try:
        feedback_files = [f for f in os.listdir(".cache/feedback") if f.endswith(".jsonl")]
        if not feedback_files:
            return
        
        total_positive = 0
        total_negative = 0
        
        for file in feedback_files:
            with open(f".cache/feedback/{file}", "r", encoding="utf-8") as f:
                for line in f:
                    data = json.loads(line)
                    if data["type"] == "positive":
                        total_positive += 1
                    elif data["type"] == "negative":
                        total_negative += 1
        
        if total_positive + total_negative > 0:
            satisfaction_rate = total_positive / (total_positive + total_negative) * 100
            st.sidebar.success(f"ğŸ“Š ë§Œì¡±ë„: {satisfaction_rate:.1f}% ({total_positive}ğŸ‘/{total_negative}ğŸ‘)")
    except:
        pass

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")

    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

    # ëª¨ë¸ ì„ íƒ ë©”ë‰´
    model_options = {
        "Gemini 2.5 Pro (ìµœê³  ì„±ëŠ¥)": "gemini-2.5-pro",
        "Gemini 2.5 Flash (ê· í˜•)": "gemini-2.5-flash", 
        "Gemini 2.5 Flash-Lite (íš¨ìœ¨)": "gemini-2.5-flash-lite",
        "Gemini 1.5 Flash (ê¸°ì¡´)": "gemini-1.5-flash"
    }
    
    selected_display = st.selectbox(
        "LLM ì„ íƒ", list(model_options.keys()), index=0
    )
    selected_model = model_options[selected_display]
    
    # ëª¨ë¸ ì„¤ëª… í‘œì‹œ
    model_descriptions = {
        "Gemini 2.5 Pro (ìµœê³  ì„±ëŠ¥)": "ğŸ† ë³µì¡í•œ ì¶”ë¡ ê³¼ ê¸´ ë¬¸ì„œ ë¶„ì„ì— ìµœì í™”",
        "Gemini 2.5 Flash (ê· í˜•)": "âš¡ ë¹ ë¥¸ ì†ë„ì™€ ì¢‹ì€ ì„±ëŠ¥ì˜ ê· í˜•",
        "Gemini 2.5 Flash-Lite (íš¨ìœ¨)": "ğŸ’° ë¹„ìš© íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ ì²˜ë¦¬",
        "Gemini 1.5 Flash (ê¸°ì¡´)": "ğŸ“‹ ê¸°ë³¸ ëª¨ë¸"
    }
    st.info(model_descriptions[selected_display])
    
    # í”¼ë“œë°± í†µê³„ í‘œì‹œ
    show_feedback_stats()
    
    # ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ ë¹„í™œì„±í™” (ì•ˆì •ì„± ìµœìš°ì„ )
    enable_query_expansion = False
    enable_hierarchical_chunking = False
    enable_reranking = False
    
    # ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ ì œê±°ë¨
    
    # ê¸°ë³¸ ê¸°ëŠ¥ë§Œ í™œì„±í™”
    features_text = "ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)"
    
    # ê¸°ë³¸ ëª¨ë“œ ì•ˆë‚´
    if uploaded_file:
        st.sidebar.info("ğŸ’¡ ê¸°ë³¸ ë²„ì „: ê°€ì¥ ì•ˆì •ì ì¸ ì„¤ì •")
    
    st.sidebar.success(f"""
ğŸš€ **ê¸°ë³¸ RAG ì‹œìŠ¤í…œ**

**ğŸ§  LLM**: Gemini 2.5 Pro
**ğŸ” ê²€ìƒ‰**: í•˜ì´ë¸Œë¦¬ë“œ (ë²¡í„° + í‚¤ì›Œë“œ)
**ğŸ—ï¸ ìƒíƒœ**: ì•ˆì • ëª¨ë“œ

{features_text}
""")
    
    # ê¸°ë³¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­
    base_improvement = 65  # Gemini 2.5 Pro + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    
    st.sidebar.metric(
        "ğŸ¯ ì„±ëŠ¥ í–¥ìƒ",
        f"+{base_improvement}%",
        "ì•ˆì •ì  ê¸°ë³¸ ë²„ì „"
    )


# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# íŒŒì¼ì„ ìºì‹œ ì €ì¥(ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)
@st.cache_resource(
    show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...",
    hash_funcs={type(st.sidebar): lambda x: None}  # ì‚¬ì´ë“œë°” ìƒíƒœ ë¬´ì‹œ
)
def embed_file(file):
    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    # ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ(Load Documents)
    loader = PDFPlumberLoader(file_path)
    docs = loader.load()
    
    # ë‹¨ê³„ 1.5: ë¬¸ì„œ ì „ì²˜ë¦¬ (í’ˆì§ˆ í–¥ìƒ)
    docs = preprocess_documents(docs)

    # ë‹¨ê³„ 2: ê¸°ë³¸ ë¬¸ì„œ ë¶„í• ë§Œ ì‚¬ìš©
    split_documents = create_simple_chunks(docs)

    # ë‹¨ê³„ 3: ì„ë² ë”©(Embedding) ìƒì„±
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    # ë‹¨ê³„ 4: ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìƒì„±
    retriever = create_hybrid_retriever(split_documents, embeddings)
    
    print(f"ğŸ Basic retriever ready: {type(retriever)}")
    return retriever


# ê¸°ë³¸ ì²´ì¸ ìƒì„± - ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ ì œê±°
def create_chain(retriever, model_name="gemini-2.5-pro"):
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = load_prompt("prompts/pdf-rag.yaml", encoding="utf-8")

    # ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±
    llm = ChatGoogleGenerativeAI(
        model=model_name, 
        temperature=0.1,
        max_tokens=2000,
        top_p=0.9
    )

    # ê¸°ë³¸ RAG ì²´ì¸ - ê°€ì¥ ì•ˆì •ì ì¸ êµ¬ì¡°
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain


# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ
if uploaded_file:
    # íŒŒì¼ ì—…ë¡œë“œ í›„ ê¸°ë³¸ retriever ìƒì„±
    retriever = embed_file(uploaded_file)
    chain = create_chain(retriever, model_name=selected_model)
    st.session_state["chain"] = chain

# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

# ë§Œì•½ì— ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...
if user_input:
    # chain ì„ ìƒì„±
    chain = st.session_state["chain"]

    if chain is not None:
        # ì‚¬ìš©ìì˜ ì…ë ¥
        st.chat_message("user").write(user_input)
        
        with st.chat_message("assistant"):
            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
            container = st.empty()
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            response = chain.stream(user_input)
            ai_answer = ""
            for token in response:
                ai_answer += token
                container.markdown(ai_answer)

        # ë‹µë³€ í’ˆì§ˆ ê²€ì¦
        quality_result = validate_answer_quality(user_input, ai_answer, "")
        
        # í’ˆì§ˆ ê²°ê³¼ í‘œì‹œ (ì‚¬ì´ë“œë°”ì—)
        with st.sidebar:
            st.write("---")
            st.write("**ğŸ“Š ë‹µë³€ í’ˆì§ˆ ë¶„ì„**")
            quality_color = "ğŸŸ¢" if quality_result["quality"] == "ë†’ìŒ" else "ğŸŸ¡" if quality_result["quality"] == "ë³´í†µ" else "ğŸ”´"
            st.write(f"{quality_color} **í’ˆì§ˆ**: {quality_result['quality']} ({quality_result['score']}/{quality_result['max_score']})")
            
            if quality_result["issues"]:
                st.write("**ê°œì„ ì :**")
                for issue in quality_result["issues"]:
                    st.write(f"â€¢ {issue}")
        
        # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
        add_message("user", user_input)
        add_message("assistant", ai_answer)
        
        # í”¼ë“œë°± ì‹œìŠ¤í…œ
        st.write("---")
        st.write("**ì´ ë‹µë³€ì´ ë„ì›€ì´ ë˜ì—ˆë‚˜ìš”?**")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ‘ ë„ì›€ë¨", key=f"positive_{len(st.session_state['messages'])}"):
                log_feedback("positive", user_input, ai_answer)
                st.success("í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤!")
                st.rerun()
        
        with col2:
            if st.button("ğŸ‘ ë„ì›€ ì•ˆë¨", key=f"negative_{len(st.session_state['messages'])}"):
                log_feedback("negative", user_input, ai_answer)
                st.info("ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•´ ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤!")
                st.rerun()
                
        with col3:
            if st.button("ğŸ“ ìƒì„¸ í”¼ë“œë°±", key=f"detailed_{len(st.session_state['messages'])}"):
                st.session_state[f"show_feedback_{len(st.session_state['messages'])}"] = True
                st.rerun()
        
        # ìƒì„¸ í”¼ë“œë°± ì…ë ¥ì°½
        if st.session_state.get(f"show_feedback_{len(st.session_state['messages'])}", False):
            with st.form(key=f"feedback_form_{len(st.session_state['messages'])}"):
                feedback_text = st.text_area("êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”:", 
                                           placeholder="ì˜ˆ: ë‹µë³€ì´ ë„ˆë¬´ ê¸¸ì–´ìš”, íŠ¹ì • ë¶€ë¶„ì´ ë¶€ì •í™•í•´ìš”, ë” ìì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•´ìš” ë“±")
                submitted = st.form_submit_button("í”¼ë“œë°± ì œì¶œ")
                
                if submitted and feedback_text:
                    log_feedback("detailed", user_input, ai_answer, feedback_text)
                    st.success("ìƒì„¸í•œ í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤! ê°œì„ ì— ì°¸ê³ í•˜ê² ìŠµë‹ˆë‹¤.")
                    st.session_state[f"show_feedback_{len(st.session_state['messages'])}"] = False
                    st.rerun()
    else:
        # íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
        warning_msg.error("íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")